# gpt-1-from scratch with pytorch
Rewriting and Pretraining GPT 1 from scratch. Implemting multihead attention (MHA) using Pytorch from: Improving Language Understanding by Generative Pre-Training (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)


This repository is solely for educational purposes. I have attempted to write the architecture and mechanism  the first model and train it on my tiny T4 GPU to see how it performs and to test how deeply i understand concepts such as multihead attention, tokenization, checkpointing, learning rate scheduling and everything in between that comes into play during model pre-training from the ground up.

Requirement for running these scripts: 
